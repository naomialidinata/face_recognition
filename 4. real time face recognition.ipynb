{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Face Recognition\n",
    "\n",
    "Let's apply all our previous knowledge into a real-time face recognition. We will try to continuously perform facial recognition on a video stream. In this example I will use an offline video as input, but in reality you can apply it using a webcam, a cctv's live camera feed, etc! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_vggface as kv\n",
    "import modules.utils as utils\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a FacePreprocess instance.\n",
    "from modules.FacePreprocess import FacePreprocess\n",
    "ssd_model = r'./models/ssd/deploy.prototxt.txt'\n",
    "ssd_weights = r'./models/ssd/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "processor = FacePreprocess(ssd_model, ssd_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the facial embedding model you want to use\n",
    "model = kv.VGGFace(\n",
    "    model='resnet50', \n",
    "    include_top=False, \n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='avg'\n",
    ")\n",
    "input_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our nmslib index tree\n",
    "nmslib_path = './output/large_scale_face_recognition/'\n",
    "\n",
    "# load id_list\n",
    "id_list = pd.read_csv(nmslib_path + '/IDlist.csv')\n",
    "\n",
    "# Euclidean distance\n",
    "index_l2 = nmslib.init(method='hnsw', space='l2', data_type=nmslib.DataType.DENSE_VECTOR)\n",
    "index_l2.loadIndex(nmslib_path + 'index_l2.bin')\n",
    "\n",
    "# Cosine similarity\n",
    "index_cos = nmslib.init(method='hnsw', space='cosinesimil', data_type=nmslib.DataType.DENSE_VECTOR)\n",
    "index_cos.loadIndex(nmslib_path + 'index_cos.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './dataset/test/test_2.mp4'\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture frame-by-frame\n",
    "\n",
    "We will use openCV's `VideoCapture` which will return the video frame by frame to try and recognize the person in the video. To test the accuracy, we will keep track of the predictions. We will also output a video showing the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    columns = ['count', 'irene', 'seulgi', 'wendy', 'joy', 'yeri'], \n",
    "    index = ['l2', 'cosinesimil']\n",
    ")\n",
    "results.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count` will keep track the number of predictions made, and the columns for the other subjects will count how many times where they predicted as the person in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean dist. `l2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = './output/real_time_face_recognition/resnet50_l2.mp4'\n",
    "videoWriter = cv2.VideoWriter(vid, fourcc, 12.0, (640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'l2'\n",
    "while (cap.isOpened()):\n",
    "    try: \n",
    "        ret, frame = cap.read()\n",
    "        img = frame.copy()\n",
    "\n",
    "        faces = processor.preproc(img)\n",
    "\n",
    "        if len(faces)>0:\n",
    "            for face in faces:\n",
    "                results['count'][dist] += 1\n",
    "\n",
    "                # target embeddings\n",
    "                target = model.predict(resize(face[0], input_size))[0,:]\n",
    "                target = np.array(target, dtype='f')\n",
    "                target = np.expand_dims(target, axis=0)\n",
    "\n",
    "                # predict\n",
    "                neighbors, distances = index_l2.knnQueryBatch(target, k=1, num_threads=4)[0]\n",
    "                \n",
    "                # results\n",
    "                name = id_list['name'][neighbors[0]]\n",
    "                results[name][dist] += 1\n",
    "\n",
    "                # add video frame\n",
    "                top, bottom, left, right = face[1][0], face[1][1], face[1][2], face[1][3]\n",
    "                cv2.putText(frame, str(name), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (255, 255, 255), 1) \n",
    "                videoWriter.write(frame)\n",
    "    except: break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine simil. `cosinesimil`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8d291e76e21ac4114bb3934f79c9cd83293577e20f5f8f5d14a21fd0b06f79d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
