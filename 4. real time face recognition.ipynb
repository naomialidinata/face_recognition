{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Face Recognition\n",
    "\n",
    "Let's apply all our previous knowledge into a real-time face recognition. We will try to continuously perform facial recognition on a video stream. In this example I will use an offline video as input, but in reality you can apply it using a webcam, a cctv's live camera feed, etc! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_vggface as kv\n",
    "import modules.utils as utils\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a FacePreprocess instance.\n",
    "from modules.FacePreprocess import FacePreprocess\n",
    "ssd_model = r'./models/ssd/deploy.prototxt.txt'\n",
    "ssd_weights = r'./models/ssd/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "processor = FacePreprocess(ssd_model, ssd_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the facial embedding model you want to use\n",
    "model = kv.VGGFace(\n",
    "    model='resnet50', \n",
    "    include_top=False, \n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='avg'\n",
    ")\n",
    "input_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our nmslib index tree\n",
    "nmslib_path = './output/large_scale_face_recognition/'\n",
    "\n",
    "# load id_list\n",
    "id_list = pd.read_csv(nmslib_path + '/IDlist.csv')\n",
    "\n",
    "# Euclidean distance\n",
    "index_l2 = nmslib.init(method='hnsw', space='l2', data_type=nmslib.DataType.DENSE_VECTOR)\n",
    "index_l2.loadIndex(nmslib_path + 'index_l2.bin')\n",
    "\n",
    "# Cosine similarity\n",
    "index_cos = nmslib.init(method='hnsw', space='cosinesimil', data_type=nmslib.DataType.DENSE_VECTOR)\n",
    "index_cos.loadIndex(nmslib_path + 'index_cos.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier evaluation, we will use a video with only one of the subjects (joy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original clip from https://youtu.be/Ia3x_X_OX58?si=aA5GdMpRGcCar2xF \n",
    "video_path = './dataset/test/test_2/joy.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture frame-by-frame\n",
    "\n",
    "We will use openCV's `VideoCapture` which will return the video frame by frame to try and recognize the person in the video. To test the accuracy, we will keep track of the predictions. We will also output a video showing the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    columns = ['count', 'irene', 'seulgi', 'wendy', 'joy', 'yeri'], \n",
    "    index = ['l2', 'cosinesimil']\n",
    ")\n",
    "results.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count` will keep track the number of predictions made, and the columns for the other subjects will count how many times where they predicted as the person in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read: https://docs.opencv.org/3.4/dd/d9e/classcv_1_1VideoWriter.html#afec93f94dc6c0b3e28f4dd153bc5a7f0 \n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean dist. `l2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'l2'\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# setup video writer\n",
    "vid = './output/real_time_face_recognition/resnet50_l2.mp4'\n",
    "out = cv2.VideoWriter() \n",
    "out.open(vid, fourcc, 24.0, (1920, 1080), True) # frame rate, frame size (w, h) must be same as input\n",
    "\n",
    "# read video frame-by-frame\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        img = frame.copy()\n",
    "        faces = processor.preproc(img)\n",
    "\n",
    "        if len(faces)>0:\n",
    "            for face in faces:\n",
    "                results['count'][dist] += 1\n",
    "\n",
    "                # target embeddings\n",
    "                target = model.predict(utils.resize(face[0], input_size), verbose=False)[0,:]\n",
    "                target = np.array(target, dtype='f')\n",
    "                target = np.expand_dims(target, axis=0)\n",
    "\n",
    "                # predict\n",
    "                neighbors, distances = index_l2.knnQueryBatch(target, k=1, num_threads=4)[0]\n",
    "                \n",
    "                # results\n",
    "                name = id_list['name'][neighbors[0]]\n",
    "                results[name][dist] += 1\n",
    "\n",
    "            # add video frame\n",
    "            top, bottom, left, right = face[1][0], face[1][1], face[1][2], face[1][3]\n",
    "            cv2.putText(frame, str(name), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (255, 255, 255), 1) \n",
    "        out.write(frame)\n",
    "\n",
    "        # if you unblock this part a window will pop up and show the frames, but only works if you run with .py \n",
    "        # On jupyter notebooks the window will crash after the loop ends and you have to restart the kernel\n",
    "        # cv2.imshow(\"frame\", frame)\n",
    "        # cv2.waitKey(1)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine simil. `cosinesimil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'cosinesimil'\n",
    "\n",
    "# load video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# setup video writer\n",
    "vid = './output/real_time_face_recognition/resnet50_cosinesimil.mp4'\n",
    "out = cv2.VideoWriter() \n",
    "out.open(vid, fourcc, 24.0, (1920, 1080), True) # frame rate, frame size (w, h) must be same as input\n",
    "\n",
    "# read video frame-by-frame\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        img = frame.copy()\n",
    "        faces = processor.preproc(img)\n",
    "\n",
    "        if len(faces)>0:\n",
    "            for face in faces:\n",
    "                results['count'][dist] += 1\n",
    "\n",
    "                # target embeddings\n",
    "                target = model.predict(utils.resize(face[0], input_size), verbose=False)[0,:]\n",
    "                target = np.array(target, dtype='f')\n",
    "                target = np.expand_dims(target, axis=0)\n",
    "\n",
    "                # predict\n",
    "                neighbors, distances = index_cos.knnQueryBatch(target, k=1, num_threads=4)[0]\n",
    "                \n",
    "                # results\n",
    "                name = id_list['name'][neighbors[0]]\n",
    "                results[name][dist] += 1\n",
    "\n",
    "            # add video frame\n",
    "            top, bottom, left, right = face[1][0], face[1][1], face[1][2], face[1][3]\n",
    "            cv2.putText(frame, str(name), (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (255, 255, 255), 1) \n",
    "        out.write(frame)\n",
    "\n",
    "        # if you unblock this part a window will pop up and show the frames, but only works if you run with .py \n",
    "        # On jupyter notebooks the window will crash after the loop ends and you have to restart the kernel\n",
    "        # cv2.imshow(\"frame\", frame)\n",
    "        # cv2.waitKey(1)\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>irene</th>\n",
       "      <th>seulgi</th>\n",
       "      <th>wendy</th>\n",
       "      <th>joy</th>\n",
       "      <th>yeri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>l2</th>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosinesimil</th>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count  irene  seulgi  wendy  joy  yeri\n",
       "l2             543      0       0      0  543     0\n",
       "cosinesimil    543      0       0      0  543     0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test video only contains Joy, so it looks like the prediction results are correct. Using both L2 and Cosine distance achieve similar results. You can also use your webcam camera! Just change the `VideoCapture` input into your webcam, train your own dataset, and you're set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook I showed how to perform facial recognition in real time. This has many real world applications, including surveillance, smart home automatic door, etc. I hope you find this useful for your projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8d291e76e21ac4114bb3934f79c9cd83293577e20f5f8f5d14a21fd0b06f79d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
